{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Req. packages - for online ipynb's\n"
   ],
   "metadata": {
    "id": "1KdgOKPQ-fB_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpKDSLIZ-dLl"
   },
   "outputs": [],
   "source": [
    "!pip install imagehash\n",
    "!pip install cairosvg\n",
    "!pip install filetype\n",
    "!pip install requests-toolbelt\n",
    "!pip install urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Put all the domains from parquet to txt"
   ],
   "metadata": {
    "id": "YBmR-G_x-inz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_domains_from_parquet(parquet_path, output_file='domains.txt'):\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if 'domain' not in df.columns:\n",
    "        raise ValueError(\"The Parquet file does not contain a 'domain' column.\")\n",
    "\n",
    "    domains = df['domain'].dropna().tolist()#extract domain values-drop NaN entries-convert to list\n",
    "\n",
    "    with open(output_file, 'w') as f: #save 2 output file\n",
    "        for domain in domains:\n",
    "            f.write(f\"{domain}\\n\")\n",
    "\n",
    "    print(f\"Successfully extracted {len(domains)} domains and saved to {output_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_domains_from_parquet('logos.snappy.parquet', 'domains.txt')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMudUDnA-mSa",
    "outputId": "95d8ec27-b2d8-4524-a615-e58c391b481b"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully extracted 4384 domains and saved to domains.txt.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extracting Logos from the domains ~4384 domains found in the parquet file\n",
    "FOUND - 2690 logos parsed"
   ],
   "metadata": {
    "id": "S79bMqfe-nro"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "\n",
    "def get_logo_url(domain):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for scheme in ['https://', 'http://']: #searched for all the types of logos\n",
    "            try:\n",
    "                url = scheme + domain\n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                og_image = soup.find('meta', property='og:image')\n",
    "                if og_image and og_image.get('content'):\n",
    "                    return urljoin(url, og_image.get('content'))\n",
    "\n",
    "                twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})\n",
    "                if twitter_image and twitter_image.get('content'):\n",
    "                    return urljoin(url, twitter_image.get('content'))\n",
    "\n",
    "                logo_selectors = [\n",
    "                    ('link', {'rel': ['icon', 'shortcut icon', 'apple-touch-icon']}),\n",
    "                    ('img', {'class': 'logo'}),\n",
    "                    ('img', {'id': 'logo'}),\n",
    "                    ('img', {'alt': 'logo'})\n",
    "                ]\n",
    "\n",
    "                for tag, attrs in logo_selectors:\n",
    "                    element = soup.find(tag, attrs)\n",
    "                    if element and element.get('href' if tag == 'link' else 'src'):\n",
    "                        return urljoin(url, element.get('href' if tag == 'link' else 'src'))\n",
    "\n",
    "            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError):\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {domain}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def download_logo(domain, output_dir=\"logos\"): #searching for the logo files in the site's header\n",
    "    logo_url = get_logo_url(domain)\n",
    "    if not logo_url:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        response = requests.get(logo_url, stream=True, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "            content_type = response.headers.get('content-type', '').split('/')[-1]\n",
    "            ext = content_type if content_type in ['png', 'jpeg', 'jpg', 'svg+xml', 'gif'] else \\\n",
    "                logo_url.split('.')[-1].split('?')[0][:4]\n",
    "            filename = f\"{domain.replace('.', '_')}.{ext.replace('svg+xml', 'svg')}\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Domain Failed\")\n",
    "    return False\n",
    "\n",
    "def process_domains(input_file=\"domains.txt\", output_dir=\"logos\"): #download the logos\n",
    "    with open(input_file, 'r') as f:\n",
    "        domains = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for domain in domains:\n",
    "        print(f\"Processing {domain}...\")\n",
    "        success = download_logo(domain, output_dir)\n",
    "        if success:\n",
    "            print(f\"Downloaded logo for {domain}\")\n",
    "        else:\n",
    "            print(f\"Failed to download logo for {domain}\")\n",
    "\n",
    "if __name__ == \"__main__\":#create logos directory if not exists\n",
    "    Path(\"logos\").mkdir(exist_ok=True)\n",
    "    process_domains()"
   ],
   "metadata": {
    "id": "HmxF8SDx-nW3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Group the logos in a json based on their predominant, secondary and tertial color scheme using Union-Find clustering with threshold and color frequency analysis"
   ],
   "metadata": {
    "id": "tLGeQ89n-5fq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_images_from_folder(folder):#load images, deliberately keep the colors\n",
    "    images = {}\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png', '.webp'}\n",
    "    for filename in os.listdir(folder):\n",
    "        ext = os.path.splitext(filename)[1].lower()\n",
    "        if ext in valid_extensions:\n",
    "            filepath = os.path.join(folder, filename)\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                images[filepath] = img\n",
    "    return images\n",
    "\n",
    "def compute_histogram(image, bins=8):#computing 3D color histogram in hsv space\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist(\n",
    "        [hsv],\n",
    "        [0, 1, 2],  #the hsv channels\n",
    "        None,\n",
    "        [bins, bins, bins],\n",
    "        [0, 180, 0, 256, 0, 256]\n",
    "    )\n",
    "    cv2.normalize(hist, hist)\n",
    "    return hist.flatten()\n",
    "\n",
    "def compute_similarity(hist1, hist2):#histogram correlation similarity\n",
    "    return cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
    "\n",
    "def cluster_images(images, similarity_threshold=0.85):#Union-Find clustering with threshold\n",
    "    paths = list(images.keys())\n",
    "    n = len(paths)\n",
    "    parent = list(range(n))\n",
    "\n",
    "    def find(u):\n",
    "        while parent[u] != u:\n",
    "            parent[u] = parent[parent[u]]\n",
    "            u = parent[u]\n",
    "        return u\n",
    "\n",
    "    def union(u, v):\n",
    "        pu, pv = find(u), find(v)\n",
    "        if pu != pv:\n",
    "            parent[pv] = pu\n",
    "\n",
    "    hists = [compute_histogram(img) for img in images.values()]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if compute_similarity(hists[i], hists[j]) >= similarity_threshold:\n",
    "                union(i, j)\n",
    "    clusters = defaultdict(list)\n",
    "    for idx in range(n):\n",
    "        clusters[find(idx)].append(paths[idx])\n",
    "\n",
    "    return list(clusters.values())\n",
    "\n",
    "def get_average_color(images): #compute circular mean for hue, arithmetic mean for saturation/value !!!!there may be some better formulas, this is the best I tried so far\n",
    "    h_angles, s_values, v_values = [], [], []\n",
    "\n",
    "    for img in images:\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        h, s, v = cv2.split(hsv)\n",
    "        h_deg = h.astype(float) * 2 #convert hue to angles (0-360)\n",
    "        sin = np.sum(np.sin(np.radians(h_deg)))\n",
    "        cos = np.sum(np.cos(np.radians(h_deg)))\n",
    "        avg_h = np.degrees(np.arctan2(sin, cos)) % 360\n",
    "\n",
    "        h_angles.append(avg_h)\n",
    "        s_values.append(np.mean(s))\n",
    "        v_values.append(np.mean(v))\n",
    "\n",
    "    sin_total = np.sum(np.sin(np.radians(h_angles)))#circular mean for hue\n",
    "    cos_total = np.sum(np.cos(np.radians(h_angles)))\n",
    "    avg_h = np.degrees(np.arctan2(sin_total, cos_total)) % 360\n",
    "\n",
    "    return {\n",
    "        \"h\": avg_h,\n",
    "        \"s\": np.mean(s_values),\n",
    "        \"v\": np.mean(v_values)\n",
    "    }\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_dominant_colors(images, n_colors=3, color_bits=5):#extract dominant colors using color frequency analysis\n",
    "\n",
    "\n",
    "    hist = {}\n",
    "    bin_size = 2 ** (8 - color_bits)#created a 3D histogram in reduced color space\n",
    "\n",
    "    for img in images:\n",
    "        resized = cv2.resize(img, (100, 100))\n",
    "        quantized = (resized // bin_size) * bin_size + bin_size//2\n",
    "        for color in quantized.reshape(-1, 3):#update histogram counts\n",
    "            bgr = tuple(color)\n",
    "            hist[bgr] = hist.get(bgr, 0) + 1\n",
    "\n",
    "    sorted_colors = sorted(hist.items(), key=lambda x: -x[1])[:n_colors]#sort colors by frequency and get top N\n",
    "    return [f\"#{r:02x}{g:02x}{b:02x}\" # Convert to hex format\n",
    "            for (b, g, r), _ in sorted_colors]\n",
    "\n",
    "def get_size_info(images): #check image sizes - fixed warnings and errors\n",
    "    if not images:  #handle empty cluster edge case\n",
    "        return \"No images\"\n",
    "\n",
    "    sizes = set(img.shape[:2][::-1] for img in images)\n",
    "\n",
    "    if len(sizes) == 1:\n",
    "        size = sizes.pop()\n",
    "        return f\"{size[0]}x{size[1]}\"\n",
    "    return f\"Varies ({len(sizes)} different sizes)\"\n",
    "\n",
    "def save_results(clusters, images, output_file=\"Color_Histogram_clusters2.json\"): #save clusters with specifications in a column\n",
    "    result = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_images = [images[path] for path in cluster]\n",
    "\n",
    "        avg_color = get_average_color(cluster_images)\n",
    "        hsv_pixel = np.uint8([[[avg_color['h']/2, avg_color['s'], avg_color['v']]]])\n",
    "        bgr_pixel = cv2.cvtColor(hsv_pixel, cv2.COLOR_HSV2BGR)\n",
    "        avg_hex = f\"#{bgr_pixel[0,0,2]:02x}{bgr_pixel[0,0,1]:02x}{bgr_pixel[0,0,0]:02x}\"\n",
    "\n",
    "        specs = {\n",
    "            \"average_color\": avg_hex,\n",
    "            \"dominant_colors\": get_dominant_colors(cluster_images),\n",
    "            \"size_info\": get_size_info(cluster_images)\n",
    "        }\n",
    "\n",
    "        result.append({\n",
    "            \"specs\": specs,\n",
    "            \"files\": [os.path.relpath(p) for p in cluster]\n",
    "        })\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FOLDER_PATH = \"/content/logos\"\n",
    "    SIMILARITY_THRESHOLD = 0.82\n",
    "\n",
    "    print(\"Loading images...\")\n",
    "    images = load_images_from_folder(FOLDER_PATH)\n",
    "\n",
    "    print(\"Clustering...\")\n",
    "    clusters = cluster_images(images, SIMILARITY_THRESHOLD)\n",
    "\n",
    "    print(\"Generating specs...\")\n",
    "    save_results(clusters, images)\n",
    "\n",
    "    print(f\"\\nSample cluster specs:\")\n",
    "    sample = json.load(open(\"Color_Histogram_clusters2.json\"))[0]\n",
    "    print(json.dumps(sample, indent=2))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHH8DMWx-6Nd",
    "outputId": "1104c5de-e2c5-406d-bcad-5ef905e1a23c"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading images...\n",
      "Clustering...\n",
      "Generating specs...\n",
      "\n",
      "Sample cluster specs:\n",
      "{\n",
      "  \"specs\": {\n",
      "    \"average_color\": \"#918894\",\n",
      "    \"dominant_colors\": [\n",
      "      \"#34343c\",\n",
      "      \"#fcfcfc\",\n",
      "      \"#f4f4f4\"\n",
      "    ],\n",
      "    \"size_info\": \"292x292\"\n",
      "  },\n",
      "  \"files\": [\n",
      "    \"logos/bakertilly_bg.png\",\n",
      "    \"logos/bakertilly_com_cy.png\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "JSON Viewer Columns List"
   ],
   "metadata": {
    "id": "KB5sRbr__GPe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import textwrap\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "COLUMN_PADDING = 40 #data configuration\n",
    "CLUSTER_SPACING = 20\n",
    "COLOR_HEIGHT = 40\n",
    "FONT_SIZE = 12\n",
    "LINE_HEIGHT = FONT_SIZE * 1.8\n",
    "TEXT_PADDING = 8\n",
    "IMAGE_SIZE = (3840, 17060)  #4K resolution !! should be adapted the width based on the number of data samples\n",
    "WRAP_WIDTH = 35\n",
    "JSON_FILE_PATH = '/content/Color_Histogram_clusters2.json'\n",
    "OUTPUT_FILE = 'Color_Histogram_clusters.png'\n",
    "MIN_CLUSTER_WIDTH = 600\n",
    "MAX_COLUMNS = math.floor((IMAGE_SIZE[0] + COLUMN_PADDING) / (MIN_CLUSTER_WIDTH + COLUMN_PADDING))\n",
    "\n",
    "def load_and_process_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    total_height = 0\n",
    "    for cluster in data:\n",
    "        wrapped_files = []\n",
    "        for file in cluster['files']:\n",
    "            wrapped_files.extend(textwrap.wrap(file, width=WRAP_WIDTH))\n",
    "        cluster['wrapped_files'] = wrapped_files\n",
    "\n",
    "        text_height = len(wrapped_files) * LINE_HEIGHT\n",
    "        cluster_height = COLOR_HEIGHT + TEXT_PADDING + text_height + TEXT_PADDING\n",
    "        cluster['height'] = cluster_height\n",
    "        total_height += cluster_height + CLUSTER_SPACING\n",
    "\n",
    "    max_column_height = IMAGE_SIZE[1] - 100\n",
    "    height_based_columns = max(1, math.ceil(total_height / max_column_height))\n",
    "    width_based_columns = MAX_COLUMNS\n",
    "    num_columns = min(height_based_columns, width_based_columns)\n",
    "\n",
    "    return data, num_columns\n",
    "\n",
    "def arrange_clusters(data, num_columns):\n",
    "    total_horizontal_space = IMAGE_SIZE[0] - (COLUMN_PADDING * (num_columns - 1))\n",
    "    column_width = max(total_horizontal_space // num_columns, MIN_CLUSTER_WIDTH)\n",
    "\n",
    "    columns = [[] for _ in range(num_columns)]\n",
    "    current_heights = [0] * num_columns\n",
    "\n",
    "    for cluster in data:\n",
    "        suitable_columns = [i for i, h in enumerate(current_heights) if h + cluster['height'] <= IMAGE_SIZE[1]]\n",
    "\n",
    "        if suitable_columns:\n",
    "            col_index = min(suitable_columns, key=lambda i: current_heights[i])\n",
    "        else:\n",
    "            col_index = current_heights.index(min(current_heights))\n",
    "            if num_columns < MAX_COLUMNS:\n",
    "                num_columns += 1\n",
    "                columns.append([])\n",
    "                current_heights.append(0)\n",
    "                col_index = num_columns - 1\n",
    "\n",
    "        columns[col_index].append(cluster)\n",
    "        current_heights[col_index] += cluster['height'] + CLUSTER_SPACING\n",
    "    column_width = max((IMAGE_SIZE[0] - (COLUMN_PADDING * (num_columns - 1))) // num_columns, MIN_CLUSTER_WIDTH)\n",
    "    x_positions = [i * (column_width + COLUMN_PADDING) for i in range(num_columns)]\n",
    "    arranged_data = []\n",
    "    for col_idx, clusters in enumerate(columns):\n",
    "        x = x_positions[col_idx]\n",
    "        y = 0\n",
    "        for cluster in clusters:\n",
    "            cluster['pos'] = (x, y)\n",
    "            cluster['width'] = column_width\n",
    "            y += cluster['height'] + CLUSTER_SPACING\n",
    "            arranged_data.append(cluster)\n",
    "\n",
    "    return arranged_data\n",
    "\n",
    "def create_visualization(data, output_filename):\n",
    "    plt.figure(figsize=(IMAGE_SIZE[0]/100, IMAGE_SIZE[1]/100), dpi=100)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(0, IMAGE_SIZE[0])\n",
    "    ax.set_ylim(IMAGE_SIZE[1], 0)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for cluster in data:\n",
    "        x, y = cluster['pos']\n",
    "        width = cluster['width']\n",
    "        height = cluster['height']\n",
    "        colors = cluster['specs']['dominant_colors']#drawing color bars\n",
    "        color_width = width / len(colors)\n",
    "        for i, color in enumerate(colors):\n",
    "            ax.add_patch(Rectangle(\n",
    "                (x + i*color_width, y), color_width, COLOR_HEIGHT,\n",
    "                facecolor=color, edgecolor='none'\n",
    "            ))\n",
    "\n",
    "        ax.add_patch(Rectangle(#container\n",
    "            (x, y), width, height,\n",
    "            linewidth=0.5, edgecolor='#333333', facecolor='none'\n",
    "        ))\n",
    "        text_y = y + COLOR_HEIGHT + TEXT_PADDING\n",
    "        for line in cluster['wrapped_files']:\n",
    "            plt.text(\n",
    "                x + TEXT_PADDING, text_y, line,\n",
    "                fontsize=FONT_SIZE, ha='left', va='top',\n",
    "                fontfamily='monospace', wrap=True,\n",
    "                color='#333333', linespacing=1.2\n",
    "            )\n",
    "            text_y += LINE_HEIGHT\n",
    "\n",
    "    plt.savefig(output_filename, bbox_inches='tight', pad_inches=0.1, dpi=100)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        cluster_data, num_cols = load_and_process_data(JSON_FILE_PATH)\n",
    "        arranged_data = arrange_clusters(cluster_data, num_cols)\n",
    "        create_visualization(arranged_data, OUTPUT_FILE)\n",
    "        print(f\"Created visualization\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2gvNCvj_FqI",
    "outputId": "8dde1716-59d4-4dde-a16d-b3fd7299c148"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created visualization\n"
     ]
    }
   ]
  }
 ]
}
