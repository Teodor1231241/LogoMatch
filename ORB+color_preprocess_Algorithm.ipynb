{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KdgOKPQ-fB_"
   },
   "source": [
    "Req. packages - for online ipynb's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MpKDSLIZ-dLl"
   },
   "outputs": [],
   "source": [
    "!pip install imagehash\n",
    "!pip install cairosvg\n",
    "!pip install filetype\n",
    "!pip install requests-toolbelt\n",
    "!pip install urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBmR-G_x-inz"
   },
   "source": [
    "Put all the domains from parquet to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMudUDnA-mSa",
    "outputId": "95d8ec27-b2d8-4524-a615-e58c391b481b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 4384 domains and saved to domains.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_domains_from_parquet(parquet_path, output_file='domains.txt'):\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if 'domain' not in df.columns:\n",
    "        raise ValueError(\"The Parquet file does not contain a 'domain' column.\")\n",
    "\n",
    "    domains = df['domain'].dropna().tolist()#extract domain values-drop NaN entries-convert to list\n",
    "\n",
    "    with open(output_file, 'w') as f: #save 2 output file\n",
    "        for domain in domains:\n",
    "            f.write(f\"{domain}\\n\")\n",
    "\n",
    "    print(f\"Successfully extracted {len(domains)} domains and saved to {output_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_domains_from_parquet('logos.snappy.parquet', 'domains.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S79bMqfe-nro"
   },
   "source": [
    "Extracting Logos from the domains ~4384 domains found in the parquet file\n",
    "FOUND - 2690 logos parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmxF8SDx-nW3"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from pathlib import Path\n",
    "\n",
    "def get_logo_url(domain):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for scheme in ['https://', 'http://']: #searched for all the types of logos\n",
    "            try:\n",
    "                url = scheme + domain\n",
    "                response = requests.get(url, headers=headers, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                og_image = soup.find('meta', property='og:image')\n",
    "                if og_image and og_image.get('content'):\n",
    "                    return urljoin(url, og_image.get('content'))\n",
    "\n",
    "                twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})\n",
    "                if twitter_image and twitter_image.get('content'):\n",
    "                    return urljoin(url, twitter_image.get('content'))\n",
    "\n",
    "                logo_selectors = [\n",
    "                    ('link', {'rel': ['icon', 'shortcut icon', 'apple-touch-icon']}),\n",
    "                    ('img', {'class': 'logo'}),\n",
    "                    ('img', {'id': 'logo'}),\n",
    "                    ('img', {'alt': 'logo'})\n",
    "                ]\n",
    "\n",
    "                for tag, attrs in logo_selectors:\n",
    "                    element = soup.find(tag, attrs)\n",
    "                    if element and element.get('href' if tag == 'link' else 'src'):\n",
    "                        return urljoin(url, element.get('href' if tag == 'link' else 'src'))\n",
    "\n",
    "            except (requests.exceptions.SSLError, requests.exceptions.ConnectionError):\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {domain}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def download_logo(domain, output_dir=\"logos\"): #searching for the logo files in the site's header\n",
    "    logo_url = get_logo_url(domain)\n",
    "    if not logo_url:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        response = requests.get(logo_url, stream=True, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "            content_type = response.headers.get('content-type', '').split('/')[-1]\n",
    "            ext = content_type if content_type in ['png', 'jpeg', 'jpg', 'svg+xml', 'gif'] else \\\n",
    "                logo_url.split('.')[-1].split('?')[0][:4]\n",
    "            filename = f\"{domain.replace('.', '_')}.{ext.replace('svg+xml', 'svg')}\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    f.write(chunk)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Domain Failed\")\n",
    "    return False\n",
    "\n",
    "def process_domains(input_file=\"domains.txt\", output_dir=\"logos\"): #download the logos\n",
    "    with open(input_file, 'r') as f:\n",
    "        domains = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for domain in domains:\n",
    "        print(f\"Processing {domain}...\")\n",
    "        success = download_logo(domain, output_dir)\n",
    "        if success:\n",
    "            print(f\"Downloaded logo for {domain}\")\n",
    "        else:\n",
    "            print(f\"Failed to download logo for {domain}\")\n",
    "\n",
    "if __name__ == \"__main__\":#create logos directory if not exists\n",
    "    Path(\"logos\").mkdir(exist_ok=True)\n",
    "    process_domains()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZsWmepYH9NQ"
   },
   "source": [
    "ORB + Color Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    filenames = []\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        if img is not None:\n",
    "            if img.ndim == 3 and img.shape[2] == 4:#handle transparency & convert to RGB\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "            if img.dtype == np.uint16:#16-bit to 8-bit - had some problems with 16 bit\n",
    "                img = (img // 256).astype(np.uint8)\n",
    "\n",
    "            img = cv2.resize(img, (200, 200))\n",
    "            filenames.append(filename)\n",
    "            images.append(img)\n",
    "    return filenames, images\n",
    "\n",
    "def extract_features(img): #Extract optimized color and shape features\n",
    "\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)#color histogram\n",
    "    color_hist = cv2.calcHist([hsv], [0, 1], None, [90, 128], [0, 180, 0, 256])\n",
    "    cv2.normalize(color_hist, color_hist)\n",
    "\n",
    "    orb = cv2.ORB_create(nfeatures=50)  #reduced keypoints\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) #shape features using ORB\n",
    "    kps = orb.detect(gray, None)\n",
    "\n",
    "    spatial_hist = np.zeros((4, 4), dtype=np.float32)#spatial hist of keypoints\n",
    "    if kps:\n",
    "        for kp in kps:\n",
    "            x, y = kp.pt\n",
    "            i = min(int(y // 50), 3)\n",
    "            j = min(int(x // 50), 3)\n",
    "            spatial_hist[i, j] += 1\n",
    "        spatial_hist /= len(kps)  # Normalize\n",
    "\n",
    "    return color_hist.flatten(), spatial_hist.flatten()\n",
    "\n",
    "def compute_similarity(args):\n",
    "\n",
    "    i, j, color_feats, shape_feats = args\n",
    "    color_sim = cv2.compareHist(color_feats[i], color_feats[j], cv2.HISTCMP_CORREL)#Parallel similarity comp\n",
    "    shape_sim = cv2.compareHist(shape_feats[i], shape_feats[j], cv2.HISTCMP_CORREL)\n",
    "    return (i, j, 1 - (0.6 * color_sim + 0.4 * shape_sim))  #weighted combination - shapes seem to be similar from one logo to another\n",
    "\n",
    "def get_cluster_description(cluster_id, members):\n",
    "    size = len(members)#generate cluster description\n",
    "    if size == 1:\n",
    "        return \"Unique item with no close matches\"\n",
    "    return f\"Group of {size} similar logos (color weight: 60%, shape weight: 40%)\" #best values for the weights\n",
    "\n",
    "def main():\n",
    "    folder = 'logos2'\n",
    "    filenames, images = load_images_from_folder(folder)\n",
    "\n",
    "\n",
    "    features = Parallel(n_jobs=-1)(delayed(extract_features)(img) for img in images) #feature extraction\n",
    "    color_feats, shape_feats = zip(*features)\n",
    "\n",
    "    n = len(filenames)#create list of pairs to compare\n",
    "    pairs = [(i, j, color_feats, shape_feats) for i in range(n) for j in range(i+1, n)]\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(delayed(compute_similarity)(pair) for pair in pairs)#similarity calc\n",
    "\n",
    "    distance_matrix = np.zeros((n, n))#build distance matrix\n",
    "    for i, j, dist in results:\n",
    "        distance_matrix[i, j] = dist\n",
    "        distance_matrix[j, i] = dist\n",
    "\n",
    "    clusterer = AgglomerativeClustering(\n",
    "        metric='precomputed',\n",
    "        linkage='average',\n",
    "        distance_threshold=0.35,\n",
    "        n_clusters=None\n",
    "    )\n",
    "    clusters = clusterer.fit_predict(distance_matrix)\n",
    "\n",
    "\n",
    "    cluster_groups = defaultdict(list)#organize clusters\n",
    "    for idx, cluster_id in enumerate(clusters):\n",
    "        cluster_groups[int(cluster_id)].append(filenames[idx])\n",
    "\n",
    "    output_data = {\n",
    "        \"metadata\": {\n",
    "            \"algorithm\": \"AgglomerativeClustering\",\n",
    "            \"parameters\": {\n",
    "                \"linkage_method\": \"average\",\n",
    "                \"distance_threshold\": 0.35,\n",
    "                \"feature_weights\": {\n",
    "                    \"color_histogram\": 0.6,\n",
    "                    \"shape_distribution\": 0.4\n",
    "                },\n",
    "                \"total_clusters\": len(cluster_groups),\n",
    "                \"total_images\": n\n",
    "            }\n",
    "        },\n",
    "        \"clusters\": []\n",
    "    }\n",
    "\n",
    "    for cluster_id, members in cluster_groups.items():\n",
    "        output_data[\"clusters\"].append({\n",
    "            \"cluster_id\": cluster_id,\n",
    "            \"member_count\": len(members),\n",
    "            \"members\": members,\n",
    "            \"characteristics\": {\n",
    "                \"description\": get_cluster_description(cluster_id, members),\n",
    "                \"average_similarity\": \"N/A\",  # Could be calculated from distance_matrix\n",
    "                \"unique_features\": [\"color\", \"shape\"]  # Based on our feature weights\n",
    "            }\n",
    "        })\n",
    "    output_file = \"ORB+color_preprocess_cluster.json\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Clustering completed. Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amIXT363Ps3y",
    "outputId": "8c935f3e-6f3e-4d77-f4d6-8c972ad41198"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Computing pairwise similarities...\n",
      "Clustering...\n",
      "Clustering completed. Results saved to cluster.json\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "JSON VIEWER W/O histogram"
   ],
   "metadata": {
    "id": "_ew8-cqGSqak"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from textwrap import wrap\n",
    "import math\n",
    "\n",
    "def visualize_clusters(json_path, output_filename):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    IMAGE_SIZE = (3840, 4000) #configure 4k picture size - should be adjusted based on the number of samples\n",
    "    DPI = 100\n",
    "    FONT_SIZE = 14\n",
    "    BOX_PADDING = 20\n",
    "    TEXT_PADDING = 10\n",
    "    LINE_HEIGHT = FONT_SIZE * 1.5\n",
    "    COLUMNS = 2  #columns for cluster boxes\n",
    "\n",
    "    plt.figure(figsize=(IMAGE_SIZE[0]/DPI, IMAGE_SIZE[1]/DPI), dpi=DPI)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(0, IMAGE_SIZE[0])\n",
    "    ax.set_ylim(IMAGE_SIZE[1], 0)\n",
    "    ax.axis('off')\n",
    "    clusters = data['clusters']\n",
    "    num_clusters = len(clusters)\n",
    "    rows = math.ceil(num_clusters / COLUMNS)\n",
    "    box_width = (IMAGE_SIZE[0] - BOX_PADDING*(COLUMNS+1)) // COLUMNS\n",
    "    box_height = (IMAGE_SIZE[1] - BOX_PADDING*(rows+1)) // rows\n",
    "    for idx, cluster in enumerate(clusters):\n",
    "        row = idx // COLUMNS\n",
    "        col = idx % COLUMNS\n",
    "        x = BOX_PADDING + col * (box_width + BOX_PADDING)\n",
    "        y = BOX_PADDING + row * (box_height + BOX_PADDING)\n",
    "        ax.add_patch(Rectangle(\n",
    "            (x, y), box_width, box_height,\n",
    "            linewidth=1, edgecolor='#333333', facecolor='none'\n",
    "        ))\n",
    "        header_text = [\n",
    "            f\"Cluster ID: {cluster['cluster_id']}\",\n",
    "            f\"Description: {cluster['characteristics']['description']}\",\n",
    "            f\"Members: {cluster['member_count']} items\",\n",
    "            f\"Features: {', '.join(cluster['characteristics']['unique_features'])}\"\n",
    "        ]\n",
    "        text_y = y + TEXT_PADDING\n",
    "        for line in header_text:\n",
    "            ax.text(\n",
    "                x + TEXT_PADDING, text_y, line,\n",
    "                fontsize=FONT_SIZE, ha='left', va='top',\n",
    "                fontfamily='monospace', color='#333333',\n",
    "                linespacing=1.2\n",
    "            )\n",
    "            text_y += LINE_HEIGHT\n",
    "        member_text = \"\\n\".join([f\"• {member}\" for member in cluster['members']])\n",
    "        wrapped_members = wrap(member_text, width=int(box_width/(FONT_SIZE*0.6)))\n",
    "        max_lines = (box_height - (text_y - y) - TEXT_PADDING) // LINE_HEIGHT\n",
    "        if len(wrapped_members) > max_lines:\n",
    "            wrapped_members = wrapped_members[:int(max_lines)] + [\n",
    "                f\"+ {len(cluster['members']) - max_lines} more items...\"\n",
    "            ]\n",
    "        text_y += LINE_HEIGHT\n",
    "        for line in wrapped_members:\n",
    "            ax.text(\n",
    "                x + TEXT_PADDING, text_y, line,\n",
    "                fontsize=FONT_SIZE, ha='left', va='top',\n",
    "                fontfamily='monospace', color='#666666',\n",
    "                linespacing=1.2\n",
    "            )\n",
    "            text_y += LINE_HEIGHT\n",
    "    plt.savefig(output_filename, bbox_inches='tight', pad_inches=0.1, dpi=DPI)\n",
    "    plt.close()\n",
    "    print(f\"Saved\")\n",
    "\n",
    "visualize_clusters(\"ORB+color_preprocess_cluster.json\", \"ORB+color_preprocess_agglomerative_clustering.png\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIgBuEfwSqKs",
    "outputId": "51807cbb-b338-44c5-d8ba-4efa19cab19d"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
